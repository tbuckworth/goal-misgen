debug:
    algo: ppo
    n_envs: 2
    n_steps: 64
    epoch: 1
    mini_batch_per_epoch: 4
    mini_batch_size: 512
    gamma: 0.999
    lmbda: 0.95
    learning_rate: 0.0005
    grad_clip_norm: 0.5
    eps_clip: 0.2
    value_coef: 0.5
    entropy_coef: 0.01
    normalize_adv: True
    normalize_rew: True
    use_gae: True
    architecture: impala
    recurrent: False

easy:
    algo: ppo
    n_envs: 64
    n_steps: 256
    epoch: 3
    mini_batch_per_epoch: 8
    mini_batch_size: 2048
    gamma: 0.999
    lmbda: 0.95
    learning_rate: 0.0005
    grad_clip_norm: 0.5
    eps_clip: 0.2
    value_coef: 0.5
    entropy_coef: 0.01
    normalize_adv: True
    normalize_rew: True
    use_gae: True
    architecture: impala
    recurrent: False

easy-200:
    algo: ppo
    n_envs: 128
    n_steps: 256
    epoch: 3
    mini_batch_per_epoch: 8
    mini_batch_size: 2048
    gamma: 0.999
    lmbda: 0.95
    learning_rate: 0.0005
    grad_clip_norm: 0.5
    eps_clip: 0.2
    value_coef: 0.5
    entropy_coef: 0.01
    normalize_adv: True
    normalize_rew: True
    use_gae: True
    architecture: impala
    recurrent: False

hard:
    algo: ppo
    n_envs: 128
    n_steps: 256
    epoch: 3
    mini_batch_per_epoch: 8
    mini_batch_size: 4096
    gamma: 0.999
    lmbda: 0.95
    learning_rate: 0.0005
    grad_clip_norm: 0.5
    eps_clip: 0.2
    value_coef: 0.5
    entropy_coef: 0.01
    normalize_adv: True
    normalize_rew: True
    use_gae: True
    architecture: impala
    recurrent: False

ascent-mlp:
    algo: ppo-lirl
    n_envs: 256
    n_steps: 256
    epoch: 3
    mini_batch_per_epoch: 8
    mini_batch_size: 8192
    gamma: 0.999
    lmbda: 0.95
    learning_rate: 0.0005
    grad_clip_norm: 0.5
    eps_clip: 0.2
    value_coef: 0.5
    entropy_coef: 0.01
    normalize_adv: True
    normalize_rew: True
    use_gae: True
    architecture: mlpmodel
    recurrent: False
    hid_dims: [3]
    final_relu: False
    l1_coef: 0.01
    anneal_lr: False
    rew_epoch: 10000
    rew_lr: 0.01
    dense_rewards: False
    num_rew_updates: 1
    n_pos_states: 20
    reset_rew_model_weights: False
    hidden_dims: [128, 128, 128]
    rew_learns_from_trusted_rollouts: False

cartpole-mlp:
    algo: ppo
    n_envs: 1024
    n_steps: 256
    epoch: 3
    mini_batch_per_epoch: 8
    mini_batch_size: 8192
    gamma: 0.999
    lmbda: 0.95
    learning_rate: 0.0005
    grad_clip_norm: 0.5
    eps_clip: 0.2
    value_coef: 0.5
    entropy_coef: 0.01
    normalize_adv: True
    normalize_rew: True
    use_gae: True
    architecture: mlpmodel
    recurrent: False
    hid_dims: [256, 256, 256]
    final_relu: False
    l1_coef: 0.
    anneal_lr: False
    train_pct_ood: 0
    reward_termination: None

cartpole-mlp-tracked:
    algo: ppo-tracked
    n_envs: 1024
    n_steps: 256
    epoch: 3
    mini_batch_per_epoch: 8
    mini_batch_size: 8192
    gamma: 0.999
    lmbda: 0.95
    learning_rate: 0.0005
    grad_clip_norm: 0.5
    eps_clip: 0.2
    value_coef: 0.5
    entropy_coef: 0.01
    normalize_adv: True
    normalize_rew: True
    use_gae: True
    architecture: mlpmodel
    recurrent: False
    hid_dims: [256, 256, 256]
    final_relu: False
    l1_coef: 0.
    anneal_lr: False
    train_pct_ood: 0
    reward_termination: None
    meg_coef: 0.
    pirc_coef: 0.

ascent-canon:
    algo: canon
    n_envs: 256
    n_steps: 256
    epoch: 3
    mini_batch_per_epoch: 8
    mini_batch_size: 8192
    gamma: 0.999
    lmbda: 0.95
    learning_rate: 0.0005
    grad_clip_norm: 0.5
    eps_clip: 0.2
    value_coef: 0.5
    entropy_coef: 0.01
    normalize_adv: True
    normalize_rew: True
    use_gae: True
    architecture: mlpmodel
    recurrent: False
    hid_dims: [3]
    final_relu: False
    anneal_lr: False
    dense_rewards: False
    num_rew_updates: 1
    n_pos_states: 20
    hidden_dims: [128, 128, 128]
    rew_learns_from_trusted_rollouts: False
    val_epoch: 10
    n_val_envs: 64
    use_unique_obs: False
    misgen: False
    load_value_models: False
    soft_canonicalisation: False
    meg: False
    value_dir: None
    remove_duplicate_actions: True
    centered_logprobs: False
    adjust_logprob_mean: False
    infinite_value: True
    meg_version: direct
    pirc: True
    trusted_policy: uniform
    trusted_temp: 2.
    subject_temp: 5.


cartpole-canon:
    algo: canon
    n_envs: 1024
    n_steps: 256
    epoch: 3
    mini_batch_per_epoch: 8
    mini_batch_size: 8192
    gamma: 0.999
    lmbda: 0.95
    learning_rate: 0.0005
    grad_clip_norm: 0.5
    eps_clip: 0.2
    value_coef: 0.5
    entropy_coef: 0.01
    normalize_adv: True
    normalize_rew: True
    use_gae: True
    architecture: mlpmodel
    recurrent: False
    hid_dims: [256, 256, 256, 256]
    final_relu: False
    anneal_lr: False
    dense_rewards: False
    num_rew_updates: 1
    rew_learns_from_trusted_rollouts: False
    val_epoch: 10
    n_val_envs: 64
    use_unique_obs: False
    misgen: False
    load_value_models: False
    soft_canonicalisation: False
    meg: False
    value_dir: None
    remove_duplicate_actions: True
    centered_logprobs: False
    adjust_logprob_mean: False
    infinite_value: True
    meg_version: direct
    pirc: True
    trusted_policy: uniform
    meg_ground_next: True
    consistency_coef: 10.
    trusted_temp: 2.
    subject_temp: 5.


trusted-value:
    algo: trusted-value
    n_envs: 256
    n_steps: 256
    mini_batch_per_epoch: 8
    mini_batch_size: 8192
    gamma: 0.999
    lmbda: 0.95
    learning_rate: 0.0005
    grad_clip_norm: 0.5
    eps_clip: 0.2
    normalize_adv: True
    normalize_rew: True
    use_gae: True
    architecture: mlpmodel
    final_relu: False
    dense_rewards: False
    n_pos_states: 20
    hidden_dims: [ 128, 128, 128 ]
    val_epoch: 1000
    n_val_envs: 64
    save_pics_ascender: False
    td_lmbda: True
    trusted_policy: uniform
    trusted_temp: 2.
    subject_temp: 5.


ascent-crafted:
    algo: canon
    n_envs: 256
    n_steps: 256
    epoch: 3
    mini_batch_per_epoch: 8
    mini_batch_size: 8192
    gamma: 0.999
    lmbda: 0.95
    learning_rate: 0.0005
    grad_clip_norm: 0.5
    eps_clip: 0.2
    value_coef: 0.5
    entropy_coef: 0.01
    normalize_adv: True
    normalize_rew: True
    use_gae: True
    architecture: crafted-policy
    misgen: True
    recurrent: False
    hid_dims: [ 3 ]
    final_relu: False
    anneal_lr: False
    dense_rewards: False
    n_pos_states: 20
    hidden_dims: [ 128, 128, 128 ]

hard-500:
    algo: ppo
    n_envs: 256 
    n_steps: 256
    epoch: 3
    mini_batch_per_epoch: 8
    mini_batch_size: 8192
    gamma: 0.999
    lmbda: 0.95
    learning_rate: 0.0005
    grad_clip_norm: 0.5
    eps_clip: 0.2
    value_coef: 0.5
    entropy_coef: 0.01
    normalize_adv: True
    normalize_rew: True
    use_gae: True
    architecture: impala
    recurrent: False

hard-500-bpo:
    algo: bpo
    n_envs: 256
    n_steps: 256
    epoch: 3
    mini_batch_per_epoch: 32
    mini_batch_size: 8192
    gamma: 0.999
    lmbda: 0.95
    learning_rate: 0.0005
    grad_clip_norm: 0.5
    eps_clip: 0.2
    value_coef: 10.
    entropy_coef: 0.
    normalize_adv: True
    normalize_rew: True
    use_gae: True
    architecture: impala
    recurrent: False
    bpo_clip: 0.5

cartpole-bpo:
    algo: bpo
    n_envs: 1024
    n_steps: 256
    epoch: 3
    mini_batch_per_epoch: 8
    mini_batch_size: 8192
    gamma: 0.999
    lmbda: 0.95
    learning_rate: 0.0005
    grad_clip_norm: 0.5
    eps_clip: 0.2
    value_coef: 0.5
    entropy_coef: 0.
    normalize_adv: True
    normalize_rew: True
    use_gae: True
    architecture: mlpmodel
    recurrent: False
    hid_dims: [256, 256, 256]
    final_relu: False
    l1_coef: 0.
    anneal_lr: False
    train_pct_ood: 0
    reward_termination: None
    bpo_clip: 0.5

hard-500-low-mem:
    algo: ppo
    n_envs: 256
    n_steps: 256
    epoch: 3
    mini_batch_per_epoch: 32
    mini_batch_size: 8192
    gamma: 0.999
    lmbda: 0.95
    learning_rate: 0.0005
    grad_clip_norm: 0.5
    eps_clip: 0.2
    value_coef: 0.5
    entropy_coef: 0.01
    normalize_adv: True
    normalize_rew: True
    use_gae: True
    architecture: impala
    recurrent: False

hard-500-low-mem-uniform:
    algo: ppo-uniform
    n_envs: 256
    n_steps: 256
    epoch: 3
    mini_batch_per_epoch: 32
    mini_batch_size: 8192
    gamma: 0.999
    lmbda: 0.95
    learning_rate: 0.0005
    grad_clip_norm: 0.5
    eps_clip: 0.2
    value_coef: 0.5
    entropy_coef: 0.01
    normalize_adv: True
    normalize_rew: True
    use_gae: True
    architecture: impala
    recurrent: False

hard-500-mem:
    algo: ppo
    n_envs: 256
    n_steps: 256
    epoch: 3
    mini_batch_per_epoch: 8
    mini_batch_size: 8192
    gamma: 0.999
    lmbda: 0.95
    learning_rate: 0.0005
    grad_clip_norm: 0.5
    eps_clip: 0.2
    value_coef: 0.5
    entropy_coef: 0.01
    normalize_adv: True
    normalize_rew: True
    use_gae: True
    architecture: impala
    recurrent: False

hard-rec:
    algo: ppo
    n_envs: 256
    n_steps: 256
    epoch: 3
    mini_batch_per_epoch: 8
    mini_batch_size: 8192
    gamma: 0.999
    lmbda: 0.95
    learning_rate: 0.0005
    grad_clip_norm: 0.5
    eps_clip: 0.2
    value_coef: 0.5
    entropy_coef: 0.01
    normalize_adv: True
    normalize_rew: True
    use_gae: True
    architecture: impala
    recurrent: True

hard-local-dev:
    algo: ppo
    n_envs: 16
    n_steps: 256
    epoch: 3
    mini_batch_per_epoch: 8
    mini_batch_size: 8192
    gamma: 0.999
    lmbda: 0.95
    learning_rate: 0.0005
    grad_clip_norm: 0.5
    eps_clip: 0.2
    value_coef: 0.5
    entropy_coef: 0.01
    normalize_adv: True
    normalize_rew: True
    use_gae: True
    architecture: impala
    recurrent: False

hard-local-dev-rec:
    algo: ppo
    n_envs: 16
    n_steps: 256
    epoch: 3
    mini_batch_per_epoch: 8
    mini_batch_size: 8192
    gamma: 0.999
    lmbda: 0.95
    learning_rate: 0.0005
    grad_clip_norm: 0.5
    eps_clip: 0.2
    value_coef: 0.5
    entropy_coef: 0.01
    normalize_adv: True
    normalize_rew: True
    use_gae: True
    architecture: impala
    recurrent: True

A100:
    algo: ppo
    n_envs: 512
    n_steps: 256
    epoch: 3
    mini_batch_per_epoch: 16
    mini_batch_size: 32768 # 32768  # this is just a maximum
    gamma: 0.999
    lmbda: 0.95
    learning_rate: 0.0005 # should make larger?
    grad_clip_norm: 0.5
    eps_clip: 0.2
    value_coef: 0.5
    entropy_coef: 0.01
    normalize_adv: True
    normalize_rew: True
    use_gae: True
    architecture: impala
    recurrent: False


A100-large:  # for larger model (16x params)
    algo: ppo
    n_envs: 512
    n_steps: 256
    epoch: 3
    mini_batch_per_epoch: 16
    mini_batch_size: 2048  # vary this param to adjust for memory
    gamma: 0.999
    lmbda: 0.95
    learning_rate: 0.0005 # scale by 1 / sqrt(channel_scale)
    grad_clip_norm: 0.5
    eps_clip: 0.2
    value_coef: 0.5
    entropy_coef: 0.01
    normalize_adv: True
    normalize_rew: True
    use_gae: True
    architecture: impala
    recurrent: False
